{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessery libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ivanyanakiev1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from inverted_index import InvertedIndex\n",
    "import nltk\n",
    "from utils import read_data\n",
    "nltk.download('stopwords')\n",
    "inv_ind = InvertedIndex()\n",
    "\n",
    "# Initialization done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will now proceed to read the documents from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = read_data(\"./shakespeare\")\n",
    "# Print the first 1 documents\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the number of the documents as well as their document title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "for i in documents:\n",
    "    # Print document title\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add documents to the inverted matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in documents:\n",
    "    # Add document to inverted index\n",
    "    inv_ind.add_document(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print come descriptives so that we can verify everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19202\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "print(inv_ind.get_total_terms())\n",
    "print(inv_ind.get_total_docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a term by document matrix using log entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Log-Entropy and the 2 components of it\n",
    "Log-Entropy is a statistical analysis of probabilities and calculation of a surprise \"index\" when certain event occurs. For example \n",
    "if a certain event has 90% chance to occur then the Log-Entropy of that event will be low since the surprise factor will be low.\n",
    "\n",
    "### Component 1\n",
    "1. This is the logarithm of the term frequency of i in document j. The term frequency can be better described as the probability of term i to occur in document j.\n",
    "Since this term occurs multiple times throughout one document (potentially or 0) we will need to multiply the natural log of it to the next component.\n",
    "\n",
    "### Component 2\n",
    "2. The second term can be interpreted as the actual amount of surprise given a discrete variable X and it's probability P(X). In order to compute the surprise\n",
    "we need the frequency of the term in the current document in regards to the total fequency.This number will of course be less than 1 and can be interpreted as yet another probability of occurance of the discrete variable X or in our case the term. We then divide by the log of the total number of documents and this final value tha we would have would represent the surprise of occurance of term i. If surprise is low then probability was high, if surprise is high then probability was low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ind.calcLogEntropy()\n",
    "inv_ind.generate_term_by_doc_matrix(log_entropy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a search query: \"scotland kings and thanes\" using the Log-Entropy weighting scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08151598183898823)\n",
      "('King Henry VI', 0.050692778961414144)\n",
      "('King Henry IV', 0.04846010673218551)\n",
      "('King Henry IV, II', 0.04471777727540368)\n",
      "('King Henry V', 0.04278470585651666)\n",
      "('King Richard III', 0.04184571614530191)\n",
      "('King John', 0.04178235330165983)\n",
      "('King Richard II', 0.04077844586842767)\n",
      "('King Henry VIII', 0.0394386617774299)\n",
      "(\"All's Well that Ends Well\", 0.038715761263254)\n"
     ]
    }
   ],
   "source": [
    "result = inv_ind.search(\"scotland kings and thanes\",log_entropy=True,cos_com = True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate term by document matrix without using TF model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ind.generate_term_by_doc_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the same data set with the same query and print top 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('King Henry V', 0.2659215354074205)\n",
      "('King Henry VI', 0.2617837075388672)\n",
      "('King John', 0.2472493685181954)\n",
      "('King Richard II', 0.2253953514359277)\n",
      "('King Lear', 0.20415867029886436)\n",
      "('King Henry VIII', 0.1998881836179047)\n",
      "('King Richard III', 0.18418950223762484)\n",
      "('Hamlet', 0.1241932011402115)\n",
      "(\"All's Well that Ends Well\", 0.11190811971898096)\n",
      "('King Henry IV', 0.10791586179996365)\n"
     ]
    }
   ],
   "source": [
    "result = inv_ind.search(\"scotland kings and thanes\",cos_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate term by doc matrix using the TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ind.calcTFIDF()\n",
    "inv_ind.generate_term_by_doc_matrix(tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test again on the same query and print the top 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08559316237351267)\n",
      "('King Henry IV', 0.005789261723483593)\n",
      "('King Henry VI', 0.003660436049077642)\n",
      "('King Henry IV, II', 0.003121709934588564)\n",
      "('King Henry V', 0.0019193400093131976)\n",
      "('King Richard III', 0.0013431147327243318)\n",
      "('King John', 0.0007488196759316429)\n",
      "('King Richard II', 0.0006742482860831404)\n",
      "('King Henry VIII', 0.0005161221482695165)\n",
      "('The Comedy of Errors', 0.00046244490997942336)\n"
     ]
    }
   ],
   "source": [
    "result = inv_ind.search(\"scotland kings and thanes\",tfidf= True,cos_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does Log-Entropy work better than or worse than TF and TF-IDf\n",
    "1. It performs better than TF. Although from the results we can see that TF clearly has the higher values TF tends to favor longer documents because of how likely there is for a term to be repeated. Longer document can contain higher frequency of the term but this strategy can quickly results in weird results.\n",
    "2. Log-Entropy is calculated based on probabilities and tries to determine the surprise of seeing a term, in its calculation more factors are accounted for\n",
    "such as frequency in current document or we can refer to this as local frequencey, and also a global frequency. It takes the inverse of those in order to calculate the overall surprise of seeing this term in the document.\n",
    "3. From looking at the test data we can infer that TF-IDF manages to guess correctly the first result \"Macbeth\" however for the  lower ones the values are 10 times smaller than the ones produced by the Log-Entropy-model we can observe that for \"example King Henry IV\" has value of 0.0057.. when using TF-IDf and the same document has 0.0506.. when using the Log-Entropy model. this is consistent throughout the results and can conclude thjat Log-Entropy performs better\n",
    "They are very similar in nature and the rankings are also nearly the same only with few rotations here and there.\n",
    "\n",
    "### Conclusion\n",
    "From this we can conclude that the Log-Entropy model performs better for information retrieval task than both IF and TF-IDF model. The TF model is a subject for potential outliers in the results and specifically in our case due to the lenght of the documents the results cannot be trusted. By comparing the results of the TF-IDF model and the Log-Entropy one we can conclude that Log-Entropy performs better in the tens of times when retrieving the value. Although the document names have the same names in the top 10, their respective values are different. Due to the nature of the Log-Entropy algorithm and taking into account both the local document frequency and the global one. By taking the inverse and calculating for the surprise we are abel to extract more accurate results and thus perform better at the taks of information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of part B Comparision Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the \"scotland kings and thanes\" but using TF method with Euclidian Distance as comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('King Henry V', 0.2659215354074205)\n",
      "('King Henry VI', 0.2617837075388672)\n",
      "('King John', 0.2472493685181954)\n",
      "('King Richard II', 0.2253953514359277)\n",
      "('King Lear', 0.20415867029886436)\n",
      "('King Henry VIII', 0.1998881836179047)\n",
      "('King Richard III', 0.18418950223762484)\n",
      "('Hamlet', 0.1241932011402115)\n",
      "(\"All's Well that Ends Well\", 0.11190811971898096)\n",
      "('King Henry IV', 0.10791586179996365)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.generate_term_by_doc_matrix()\n",
    "result = inv_ind.search(\"scotland kings and thanes\",cos_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for \"scotland kings and thanes\" using TF method with Pearson Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('King Henry V', 0.2659215354074205)\n",
      "('King Henry VI', 0.2617837075388672)\n",
      "('King John', 0.2472493685181954)\n",
      "('King Richard II', 0.2253953514359277)\n",
      "('King Lear', 0.20415867029886436)\n",
      "('King Henry VIII', 0.1998881836179047)\n",
      "('King Richard III', 0.18418950223762484)\n",
      "('Hamlet', 0.1241932011402115)\n",
      "(\"All's Well that Ends Well\", 0.11190811971898096)\n",
      "('King Henry IV', 0.10791586179996365)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.generate_term_by_doc_matrix()\n",
    "result = inv_ind.search(\"scotland kings and thanes\",pear_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using TF method with Spearman Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('King Henry V', 0.2659215354074205)\n",
      "('King Henry VI', 0.2617837075388672)\n",
      "('King John', 0.2472493685181954)\n",
      "('King Richard II', 0.2253953514359277)\n",
      "('King Lear', 0.20415867029886436)\n",
      "('King Henry VIII', 0.1998881836179047)\n",
      "('King Richard III', 0.18418950223762484)\n",
      "('Hamlet', 0.1241932011402115)\n",
      "(\"All's Well that Ends Well\", 0.11190811971898096)\n",
      "('King Henry IV', 0.10791586179996365)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.generate_term_by_doc_matrix()\n",
    "result = inv_ind.search(\"scotland kings and thanes\",spear_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using TF method with Kendalltau Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('King Henry V', 0.2659215354074205)\n",
      "('King Henry VI', 0.2617837075388672)\n",
      "('King John', 0.2472493685181954)\n",
      "('King Richard II', 0.2253953514359277)\n",
      "('King Lear', 0.20415867029886436)\n",
      "('King Henry VIII', 0.1998881836179047)\n",
      "('King Richard III', 0.18418950223762484)\n",
      "('Hamlet', 0.1241932011402115)\n",
      "(\"All's Well that Ends Well\", 0.11190811971898096)\n",
      "('King Henry IV', 0.10791586179996365)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.generate_term_by_doc_matrix()\n",
    "result = inv_ind.search(\"scotland kings and thanes\",kend_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using TF-IDF method with Cosine comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08559316237351267)\n",
      "('King Henry IV', 0.005789261723483593)\n",
      "('King Henry VI', 0.003660436049077642)\n",
      "('King Henry IV, II', 0.003121709934588564)\n",
      "('King Henry V', 0.0019193400093131976)\n",
      "('King Richard III', 0.0013431147327243318)\n",
      "('King John', 0.0007488196759316429)\n",
      "('King Richard II', 0.0006742482860831404)\n",
      "('King Henry VIII', 0.0005161221482695165)\n",
      "('The Comedy of Errors', 0.00046244490997942336)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcTFIDF()\n",
    "inv_ind.generate_term_by_doc_matrix(tfidf=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",tfidf=True,cos_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using TF-IDF method with Euclidian Distance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08559316237351267)\n",
      "('King Henry IV', 0.005789261723483593)\n",
      "('King Henry VI', 0.003660436049077642)\n",
      "('King Henry IV, II', 0.003121709934588564)\n",
      "('King Henry V', 0.0019193400093131976)\n",
      "('King Richard III', 0.0013431147327243318)\n",
      "('King John', 0.0007488196759316429)\n",
      "('King Richard II', 0.0006742482860831404)\n",
      "('King Henry VIII', 0.0005161221482695165)\n",
      "('The Comedy of Errors', 0.00046244490997942336)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcTFIDF()\n",
    "inv_ind.generate_term_by_doc_matrix(tfidf=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",tfidf=True,euc_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for  \"scotland kings and thanes \" using TF-IDF method with Pearson Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08559316237351267)\n",
      "('King Henry IV', 0.005789261723483593)\n",
      "('King Henry VI', 0.003660436049077642)\n",
      "('King Henry IV, II', 0.003121709934588564)\n",
      "('King Henry V', 0.0019193400093131976)\n",
      "('King Richard III', 0.0013431147327243318)\n",
      "('King John', 0.0007488196759316429)\n",
      "('King Richard II', 0.0006742482860831404)\n",
      "('King Henry VIII', 0.0005161221482695165)\n",
      "('The Comedy of Errors', 0.00046244490997942336)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcTFIDF()\n",
    "inv_ind.generate_term_by_doc_matrix(tfidf=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",tfidf=True,pear_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using TF-IDF method with Spearman Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08559316237351267)\n",
      "('King Henry IV', 0.005789261723483593)\n",
      "('King Henry VI', 0.003660436049077642)\n",
      "('King Henry IV, II', 0.003121709934588564)\n",
      "('King Henry V', 0.0019193400093131976)\n",
      "('King Richard III', 0.0013431147327243318)\n",
      "('King John', 0.0007488196759316429)\n",
      "('King Richard II', 0.0006742482860831404)\n",
      "('King Henry VIII', 0.0005161221482695165)\n",
      "('The Comedy of Errors', 0.00046244490997942336)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcTFIDF()\n",
    "inv_ind.generate_term_by_doc_matrix(tfidf=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",tfidf=True,spear_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using TF-IDF method with Kendalltau Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08559316237351267)\n",
      "('King Henry IV', 0.005789261723483593)\n",
      "('King Henry VI', 0.003660436049077642)\n",
      "('King Henry IV, II', 0.003121709934588564)\n",
      "('King Henry V', 0.0019193400093131976)\n",
      "('King Richard III', 0.0013431147327243318)\n",
      "('King John', 0.0007488196759316429)\n",
      "('King Richard II', 0.0006742482860831404)\n",
      "('King Henry VIII', 0.0005161221482695165)\n",
      "('The Comedy of Errors', 0.00046244490997942336)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcTFIDF()\n",
    "inv_ind.generate_term_by_doc_matrix(tfidf=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",tfidf=True,kend_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using Log-Entropy with Cosine comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08151598183898823)\n",
      "('King Henry VI', 0.050692778961414144)\n",
      "('King Henry IV', 0.04846010673218551)\n",
      "('King Henry IV, II', 0.04471777727540368)\n",
      "('King Henry V', 0.04278470585651666)\n",
      "('King Richard III', 0.04184571614530191)\n",
      "('King John', 0.04178235330165983)\n",
      "('King Richard II', 0.04077844586842767)\n",
      "('King Henry VIII', 0.0394386617774299)\n",
      "(\"All's Well that Ends Well\", 0.038715761263254)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcLogEntropy()\n",
    "inv_ind.generate_term_by_doc_matrix(log_entropy=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",log_entropy=True,cos_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using Log-Entropy with Pearson Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08151598183898823)\n",
      "('King Henry VI', 0.050692778961414144)\n",
      "('King Henry IV', 0.04846010673218551)\n",
      "('King Henry IV, II', 0.04471777727540368)\n",
      "('King Henry V', 0.04278470585651666)\n",
      "('King Richard III', 0.04184571614530191)\n",
      "('King John', 0.04178235330165983)\n",
      "('King Richard II', 0.04077844586842767)\n",
      "('King Henry VIII', 0.0394386617774299)\n",
      "(\"All's Well that Ends Well\", 0.038715761263254)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcLogEntropy()\n",
    "inv_ind.generate_term_by_doc_matrix(log_entropy=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",log_entropy=True,pear_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using Log-Entropy with Spearman Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08151598183898823)\n",
      "('King Henry VI', 0.050692778961414144)\n",
      "('King Henry IV', 0.04846010673218551)\n",
      "('King Henry IV, II', 0.04471777727540368)\n",
      "('King Henry V', 0.04278470585651666)\n",
      "('King Richard III', 0.04184571614530191)\n",
      "('King John', 0.04178235330165983)\n",
      "('King Richard II', 0.04077844586842767)\n",
      "('King Henry VIII', 0.0394386617774299)\n",
      "(\"All's Well that Ends Well\", 0.038715761263254)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcLogEntropy()\n",
    "inv_ind.generate_term_by_doc_matrix(log_entropy=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",log_entropy=True,spear_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for \"scotland kings and thanes\" using Log-Entropy with Kendalltau Correlation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08151598183898823)\n",
      "('King Henry VI', 0.050692778961414144)\n",
      "('King Henry IV', 0.04846010673218551)\n",
      "('King Henry IV, II', 0.04471777727540368)\n",
      "('King Henry V', 0.04278470585651666)\n",
      "('King Richard III', 0.04184571614530191)\n",
      "('King John', 0.04178235330165983)\n",
      "('King Richard II', 0.04077844586842767)\n",
      "('King Henry VIII', 0.0394386617774299)\n",
      "(\"All's Well that Ends Well\", 0.038715761263254)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcLogEntropy()\n",
    "inv_ind.generate_term_by_doc_matrix(log_entropy=True)\n",
    "result = inv_ind.search(\"scotland kings and thanes\",log_entropy=True,kend_com=True)\n",
    "for i in range(0,10):\n",
    "    print(result[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for  \"scotland kings and thanes\" using Log-Entropy with Euclidian comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Macbeth', 0.08151598183898823)\n",
      "('King Henry VI', 0.050692778961414144)\n",
      "('King Henry IV', 0.04846010673218551)\n",
      "('King Henry IV, II', 0.04471777727540368)\n",
      "('King Henry V', 0.04278470585651666)\n",
      "('King Richard III', 0.04184571614530191)\n",
      "('King John', 0.04178235330165983)\n",
      "('King Richard II', 0.04077844586842767)\n",
      "('King Henry VIII', 0.0394386617774299)\n",
      "(\"All's Well that Ends Well\", 0.038715761263254)\n"
     ]
    }
   ],
   "source": [
    "inv_ind.calcLogEntropy()\n",
    "inv_ind.generate_term_by_doc_matrix(log_entropy=True)\n",
    "results =inv_ind.search(\"scotland kings and thanes\",log_entropy=True,euc_com=True)\n",
    "for i in range(0,10):\n",
    "    print(results[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from the overall experiment\n",
    "The comparison operators that we have implemented are as follows:\n",
    "1. Spearman\n",
    "2. Pearson\n",
    "3. Kendall Tau\n",
    "4. Euclidian\n",
    "\n",
    "From using all of them we can conclude that they do not affect the results in any way and when comparing the similarities between two vectors in our case\n",
    "\"scotland kings and thames\" query vector against the ones in the inverted index, the results were the same. Results only differ from the fact if we are suing the TF,TF-IDF, or Log-Entropy model, but remain consistent within each model no matter of the operator being used. This could be due to some of the following reasons:\n",
    "\n",
    "1. Long vectors to compare so if small difference in smaller once in big ones it eventually gets nulled out\n",
    "2. Spearman,Pearson, and Kendall Tau might produce different results only if the data has different shapes, for example Spearman better than Pearson when data does not resemble a line. A lot of noise. Spearman is more resistant to that. Thus maybe it is just the specific query that is being used but generally these methods produce very similar results.\n",
    "3. In theory the euclidian distance should produce different result from them however it doesn't. From the test below we can see that only the euclidian distance produces very different result, however in our case when using it for the search query result is the same. We attribute this fact of the lengths of the query vectors. The example below is faily simple and might not show entirely correctly the whole picture.\n",
    "\n",
    "## Conclusion\n",
    "We cannot conlude that a specific method works better than the other, from what has been observed all of the methods extract the same data and perform equally well. I guess if more research is done a pattern can be found on which method potentially might perform better, since each of these methods is optimized for specific shape of the data. Thus different results might be obtained if specific queries modify the shape of the resulting query vector and the total difference in it. However, with this specific query given the results were the same so it can be concluded that all of the 4 work as expected and neither one is better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the 4 comparison operators on 2 dummy vectors to obtain results and potentially an explanation of our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746318461970762\n",
      "0.9999999999999998\n",
      "1.0\n",
      "5.196152422706632\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Testing different methods on a 2 vectors\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "print(inv_ind.cosine_comparison(a,b))\n",
    "print(inv_ind.pearson_comparison(a,b))\n",
    "print(inv_ind.spearman_comparison(a,b))\n",
    "print(inv_ind.euclidian_comparison(a,b))\n",
    "print(inv_ind.kendalltau_comparison(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From what we see 4 out of the 5 comparison methods produce near identical results, only the euclidian distance does not however in our cae we might contribute the identicity of all 5 to the length of the documents and that overall data and difference between them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
