{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "#### Different examples of tokenisation in Python.\n",
    "\n",
    "#### First, we need to import the nltk libraries and regular expression features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ivanyanakiev1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ivanyanakiev1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /home/ivanyanakiev1/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ivanyanakiev1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/ivanyanakiev1/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package crubadan to\n",
      "[nltk_data]     /home/ivanyanakiev1/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/crubadan.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('crubadan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's create some sample data to play with...\n",
    "\n",
    "#### I used used a couple of lines of text from a BBC sports page as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The European Super League (ESL) is on \"standby\" despite nine of the 12 founding ' \\\n",
    "       'teams withdrawing, says Real Madrid president Florentino Perez.  After a furious '\\\n",
    "       'backlash against the proposed tournament that was announced on Sunday, all six '\\\n",
    "       'Premier League clubs involved withdrew on Tuesday.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we can tokenise by word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Super', 'League', '(', 'ESL', ')', 'is', 'on', '``', 'standby', \"''\", 'despite', 'nine', 'of', 'the', '12', 'founding', 'teams', 'withdrawing', ',', 'says', 'Real', 'Madrid', 'president', 'Florentino', 'Perez', '.', 'After', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'Sunday', ',', 'all', 'six', 'Premier', 'League', 'clubs', 'involved', 'withdrew', 'on', 'Tuesday', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will notice in the list above it treats punctuation as individual tokens.  It's easy enough to strip this out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Super', 'League', 'ESL', 'is', 'on', 'standby', 'despite', 'nine', 'of', 'the', 'founding', 'teams', 'withdrawing', 'says', 'Real', 'Madrid', 'president', 'Florentino', 'Perez', 'After', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'Sunday', 'all', 'six', 'Premier', 'League', 'clubs', 'involved', 'withdrew', 'on', 'Tuesday']\n"
     ]
    }
   ],
   "source": [
    "test = [word for word in word_tokens if word.isalpha()]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Well, the punctuation is gone but notice the 12 disappeared?  That's because it's not an alpha character.  Let's try again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Super', 'League', 'ESL', 'is', 'on', 'standby', 'despite', 'nine', 'of', 'the', '12', 'founding', 'teams', 'withdrawing', 'says', 'Real', 'Madrid', 'president', 'Florentino', 'Perez', 'After', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'Sunday', 'all', 'six', 'Premier', 'League', 'clubs', 'involved', 'withdrew', 'on', 'Tuesday']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = [word for word in word_tokens if word.isalpha() or word.isnumeric()]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another quick and dirty way to tokenise is just to split on whitespace..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Super', 'League', '(ESL)', 'is', 'on', '\"standby\"', 'despite', 'nine', 'of', 'the', '12', 'founding', 'teams', 'withdrawing,', 'says', 'Real', 'Madrid', 'president', 'Florentino', 'Perez.', 'After', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'Sunday,', 'all', 'six', 'Premier', 'League', 'clubs', 'involved', 'withdrew', 'on', 'Tuesday.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens2 = text.split()\n",
    "print(word_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You'll notice that the punctuation is now with the individual terms, we can still strip it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Super', 'League', 'is', 'on', 'despite', 'nine', 'of', 'the', 'founding', 'teams', 'says', 'Real', 'Madrid', 'president', 'Florentino', 'After', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'all', 'six', 'Premier', 'League', 'clubs', 'involved', 'withdrew', 'on']\n"
     ]
    }
   ],
   "source": [
    "test = [word for word in word_tokens2 if word.isalpha()]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ooops, that seems to have gotten rid of any text containing punctuation, let's try again. \n",
    "\n",
    "#### We will use a regular expression so that we end up just accepting words from the list.\n",
    "\n",
    "#### Note, the 2nd parameter says \"\" will be substituted if it does satisfy the regular expression.  You'll need to use this with care with some tokenisation as you may have punctuation on its own or another anomolies that will turn into \"\" entries in your list (which is easy enough to strip out at any rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Super', 'League', 'ESL', 'is', 'on', 'standby', 'despite', 'nine', 'of', 'the', '12', 'founding', 'teams', 'withdrawing', 'says', 'Real', 'Madrid', 'president', 'Florentino', 'Perez', 'After', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'Sunday', 'all', 'six', 'Premier', 'League', 'clubs', 'involved', 'withdrew', 'on', 'Tuesday']\n"
     ]
    }
   ],
   "source": [
    "word_tokens2 = [re.sub('[^\\w]', \"\", word) for word in word_tokens2]\n",
    "print(word_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That seems to have done the trick\n",
    "\n",
    "#### You will see below that the two tokenised versions are now the same, just 2 different ways of doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'European', 'Super', 'League', 'ESL', 'is', 'on', 'standby', 'despite', 'nine', 'of', 'the', '12', 'founding', 'teams', 'withdrawing', 'says', 'Real', 'Madrid', 'president', 'Florentino', 'Perez', 'After', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'Sunday', 'all', 'six', 'Premier', 'League', 'clubs', 'involved', 'withdrew', 'on', 'Tuesday']\n",
      "['The', 'European', 'Super', 'League', 'ESL', 'is', 'on', 'standby', 'despite', 'nine', 'of', 'the', '12', 'founding', 'teams', 'withdrawing', 'says', 'Real', 'Madrid', 'president', 'Florentino', 'Perez', 'After', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'Sunday', 'all', 'six', 'Premier', 'League', 'clubs', 'involved', 'withdrew', 'on', 'Tuesday']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)\n",
    "print(word_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step will be to lower case the terms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'european', 'super', 'league', 'esl', 'is', 'on', 'standby', 'despite', 'nine', 'of', 'the', '12', 'founding', 'teams', 'withdrawing', 'says', 'real', 'madrid', 'president', 'florentino', 'perez', 'after', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'sunday', 'all', 'six', 'premier', 'league', 'clubs', 'involved', 'withdrew', 'on', 'tuesday']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = [word.lower() for word in word_tokens]\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will also undoubtably come accross other cases, like contractions or other odditites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"O'Niell\", 'ca', \"n't\", 'run', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"O'Niell can't run.\"\n",
    "text = nltk.word_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, O'Niell comes out fine, but the contraction has been split up?  This may or may not be useful.\n",
    "\n",
    "#### Some NLP tools can deal with that type of input.  I generally avoid it which is one reason why splitting on space, in some cases, can make things easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"O'Niell\", \"can't\", 'run.']\n",
      "['ONiell', 'cant', 'run']\n"
     ]
    }
   ],
   "source": [
    "text = \"O'Niell can't run.\"\n",
    "text = text.split()\n",
    "print(text)\n",
    "text = [re.sub('[^\\w]', \"\", word) for word in text]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatisation\n",
    "\n",
    "#### Let's run through some of the examples to see what happens if we stem/lemmatise them\n",
    "\n",
    "#### First, we need to import some additional nltk libraries to work with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the cases with punctuation we introduced above as well as some other \"gotcha\" cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o'niel\n",
      "can't\n",
      "cant'\n",
      "hers'\n",
      "her\n",
      "univers\n",
      "univers\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem(\"O'Niell\"))\n",
    "print(porter_stemmer.stem(\"can't\"))\n",
    "print(porter_stemmer.stem(\"cant'\"))\n",
    "print(porter_stemmer.stem(\"hers'\"))\n",
    "print(porter_stemmer.stem(\"hers\"))\n",
    "print(porter_stemmer.stem(\"university\"))\n",
    "print(porter_stemmer.stem(\"universe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's stem the longer text we tokenized earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'european', 'super', 'league', 'esl', 'is', 'on', 'standby', 'despite', 'nine', 'of', 'the', '12', 'founding', 'teams', 'withdrawing', 'says', 'real', 'madrid', 'president', 'florentino', 'perez', 'after', 'a', 'furious', 'backlash', 'against', 'the', 'proposed', 'tournament', 'that', 'was', 'announced', 'on', 'sunday', 'all', 'six', 'premier', 'league', 'clubs', 'involved', 'withdrew', 'on', 'tuesday']\n",
      "['the', 'european', 'super', 'leagu', 'esl', 'is', 'on', 'standbi', 'despit', 'nine', 'of', 'the', '12', 'found', 'team', 'withdraw', 'say', 'real', 'madrid', 'presid', 'florentino', 'perez', 'after', 'a', 'furiou', 'backlash', 'against', 'the', 'propos', 'tournament', 'that', 'wa', 'announc', 'on', 'sunday', 'all', 'six', 'premier', 'leagu', 'club', 'involv', 'withdrew', 'on', 'tuesday']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)\n",
    "stemmed_text = [porter_stemmer.stem(word) for word in word_tokens]\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's apply lemmatisation.  \n",
    "\n",
    "#### Remember, we need to the part of speech tag for this to work properly so let's get that first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('european', 'JJ'), ('super', 'NN'), ('league', 'NN'), ('esl', 'NN'), ('is', 'VBZ'), ('on', 'IN'), ('standby', 'JJ'), ('despite', 'IN'), ('nine', 'CD'), ('of', 'IN'), ('the', 'DT'), ('12', 'CD'), ('founding', 'JJ'), ('teams', 'NNS'), ('withdrawing', 'VBG'), ('says', 'VBZ'), ('real', 'JJ'), ('madrid', 'JJ'), ('president', 'NN'), ('florentino', 'NN'), ('perez', 'NN'), ('after', 'IN'), ('a', 'DT'), ('furious', 'JJ'), ('backlash', 'NN'), ('against', 'IN'), ('the', 'DT'), ('proposed', 'VBN'), ('tournament', 'NN'), ('that', 'WDT'), ('was', 'VBD'), ('announced', 'VBN'), ('on', 'IN'), ('sunday', 'NN'), ('all', 'DT'), ('six', 'CD'), ('premier', 'JJR'), ('league', 'NN'), ('clubs', 'NNS'), ('involved', 'VBN'), ('withdrew', 'NN'), ('on', 'IN'), ('tuesday', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "POSTags = nltk.pos_tag(word_tokens)\n",
    "print(POSTags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we needed to look it up, we can actually print out the meanings of the POS if you don't know them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "for t in POSTags:\n",
    "  nltk.help.upenn_tagset(t[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will need to make a translation function in order to use our POS in the WordNet Lemmatiser as it uses a different set (or subset) of POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_post(word):\n",
    "  # Remember, the word is a tuple, word[0] = word, word[1] = POS Tag\n",
    "  tag = word[1][0].upper()\n",
    "  tag_dictionary = { \"J\": wordnet.ADJ,\n",
    "\t                 \"N\": wordnet.NOUN,\n",
    "\t                 \"V\": wordnet.VERB,\n",
    "\t                 \"R\": wordnet.ADV}\n",
    "\t\n",
    "  # retrive value from dictionary, if not found use default of NOUN\n",
    "  return tag_dictionary.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can lemmatise our text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation of the sentence: \n",
      "[the]:  the which is a n\n",
      "[european]:  european which is a a\n",
      "[super]:  super which is a n\n",
      "[league]:  league which is a n\n",
      "[esl]:  esl which is a n\n",
      "[is]:  be which is a v\n",
      "[on]:  on which is a n\n",
      "[standby]:  standby which is a a\n",
      "[despite]:  despite which is a n\n",
      "[nine]:  nine which is a n\n",
      "[of]:  of which is a n\n",
      "[the]:  the which is a n\n",
      "[12]:  12 which is a n\n",
      "[founding]:  founding which is a a\n",
      "[teams]:  team which is a n\n",
      "[withdrawing]:  withdraw which is a v\n",
      "[says]:  say which is a v\n",
      "[real]:  real which is a a\n",
      "[madrid]:  madrid which is a a\n",
      "[president]:  president which is a n\n",
      "[florentino]:  florentino which is a n\n",
      "[perez]:  perez which is a n\n",
      "[after]:  after which is a n\n",
      "[a]:  a which is a n\n",
      "[furious]:  furious which is a a\n",
      "[backlash]:  backlash which is a n\n",
      "[against]:  against which is a n\n",
      "[the]:  the which is a n\n",
      "[proposed]:  propose which is a v\n",
      "[tournament]:  tournament which is a n\n",
      "[that]:  that which is a n\n",
      "[was]:  be which is a v\n",
      "[announced]:  announce which is a v\n",
      "[on]:  on which is a n\n",
      "[sunday]:  sunday which is a n\n",
      "[all]:  all which is a n\n",
      "[six]:  six which is a n\n",
      "[premier]:  premier which is a a\n",
      "[league]:  league which is a n\n",
      "[clubs]:  club which is a n\n",
      "[involved]:  involve which is a v\n",
      "[withdrew]:  withdrew which is a n\n",
      "[on]:  on which is a n\n",
      "[tuesday]:  tuesday which is a n\n"
     ]
    }
   ],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "print(\"Lemmatisation of the sentence: \")\n",
    "for t in POSTags:\n",
    "  term = t[0]\n",
    "  print(\"[\" + term + \"]:  \" + lemmatiser.lemmatize(term, pos = get_wordnet_post(t)) + \\\n",
    "        \" which is a \" + get_wordnet_post(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, using a lemmatiser is somewhat more work - in practice I find results between stemming/lemmatisation are usually pretty similar, not much to choose between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, an example of what happens if you *don't* use a POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better : good\n",
      "better : better\n"
     ]
    }
   ],
   "source": [
    "print(\"better :\", lemmatiser.lemmatize(\"better\", pos =\"a\"))\n",
    "print(\"better :\", lemmatiser.lemmatize(\"better\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection\n",
    "\n",
    "#### To cap things off, we will look at a model that guesses the language of text.  \n",
    "\n",
    "#### First, let's come up with some sample pieces of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "#English\n",
    "phrase_one = \"good morning\"\n",
    "# Afrikaans\n",
    "phrase_two = \"goeie more\"\n",
    "# Italian\n",
    "phrase_three = \"buongiorno\"\n",
    "# Korean\n",
    "phrase_four = \"좋은 아침\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we will instantiate a text classificaiton model and see what we come up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n",
      "afr\n",
      "ita\n",
      "abk\n",
      "English\n",
      "Afrikaans\n",
      "Italian\n",
      "Abkhazian\n"
     ]
    }
   ],
   "source": [
    "tc = nltk.classify.textcat.TextCat() \n",
    "guess_one = tc.guess_language(phrase_one)\n",
    "guess_two = tc.guess_language(phrase_two)\n",
    "guess_three = tc.guess_language(phrase_three)\n",
    "guess_four = tc.guess_language(phrase_four)\n",
    "\n",
    "print(guess_one)\n",
    "print(guess_two)\n",
    "print(guess_three)\n",
    "print(guess_four)\n",
    "\n",
    "guess_one_name = pycountry.languages.get(alpha_3=guess_one).name\n",
    "guess_two_name = pycountry.languages.get(alpha_3=guess_two).name\n",
    "guess_three_name = pycountry.languages.get(alpha_3=guess_three).name\n",
    "guess_four_name = pycountry.languages.get(alpha_3=guess_four).name\n",
    "print(guess_one_name)\n",
    "print(guess_two_name)\n",
    "print(guess_three_name)\n",
    "print(guess_four_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
